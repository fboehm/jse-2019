


```{r, recover-{{date}}}
fn <- paste0("../data/", "{{date}}", "-analyzed-tweet-ids.csv")
#fn <- paste0("../data/", "2020-05-10", "-analyzed-tweet-ids.csv")
tr_fn <- paste0("../data/", "{{date}}", "-rehydrated-tweets.csv")
#tr_fn <- paste0("../data/", "2020-05-10", "-rehydrated-tweets.csv")
if (!file.exists(tr_fn)){
  tweets <- recover_stream(path = "{{tweet_json_file}}", verbose = TRUE) 
  #tweets <- recover_stream(path = "../data/2020-05-10 12:00:01-tweets.json", verbose = TRUE) 
  id_fn <- paste0("../data/tweets-status-ids-", "{{date}}", ".csv")
  tweets %>%
    dplyr::select(status_id) %>%
    readr::write_csv(path = id_fn, 
                     col_names = FALSE)
  broken_ids <- readr::read_csv("broken_tweets.txt", col_names = FALSE) %>%
    dplyr::rename(status_id = X1) %>%
    dplyr::mutate(status_id = trimws(format(status_id, scientific = FALSE)))
  ids <- readr::read_csv(id_fn, col_names = FALSE) %>%
    dplyr::rename(status_id = X1) %>%
    dplyr::mutate(status_id = as.character(status_id))
  tweets_rehydrated1 <- rtweet::lookup_tweets(statuses = ids$status_id, 
                                              parse = TRUE)
  tweets_rehydrated2 <- rtweet::lookup_tweets(statuses = broken_ids$status_id, parse = TRUE)
  tweets_rehydrated <- tweets_rehydrated1 %>%
    dplyr::bind_rows(tweets_rehydrated2)
  # save ids of tweets that we actually used
  readr::write_csv(tibble::as_tibble(tweets_rehydrated$status_id), path = fn, col_names = FALSE)
  readr::write_csv(tweets_rehydrated, path = tr_fn)
} else {
  tweets_rehydrated <- readr::read_csv(tr_fn)
}
```


## Prepare document-term matrix from tweet words


```{r, wordcounts-{{date}}}
# https://www.tidytextmining.com/topicmodeling.html#library-heist
# https://www.tidytextmining.com/twitter.html
word_counts <- tweets_rehydrated %>% 
  dplyr::filter(lang == "en") %>%
  dplyr::filter(!stringr::str_detect(text, "^RT")) %>%
  dplyr::mutate(text = stringr::str_remove_all(text, remove_reg)) %>%
  tidytext::unnest_tokens(output = word, input = text) %>%
  dplyr::select(status_id, word) %>%
  dplyr::anti_join(stop_words) %>%
  dplyr::anti_join(twitter_stop) %>%
  dplyr::filter(word %in% GradyAugmented) %>%
  dplyr::count(status_id, word, sort = TRUE) %>%
  dplyr::ungroup()
```

```{r, cast-{{date}}}
tweets_dtm <- word_counts %>%
  tidytext::cast_dtm(status_id, word, n)
```

```{r, lda-{{date}}}
tweets_lda <- topicmodels::LDA(tweets_dtm, k = 8)
tweets_beta <- tidytext::tidy(tweets_lda, matrix = "beta")
```

```{r, topterms-{{date}}}
tweets_top_terms <- tweets_beta %>%
  dplyr::group_by(topic) %>%
  dplyr::top_n(10, beta) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta)
```

```{r, plot-{{date}}}
tweets_top_terms %>%
  dplyr::mutate(term = tidytext::reorder_within(term, beta, topic)) %>%
  ggplot2::ggplot(ggplot2::aes(term, beta, fill = factor(topic))) +
  ggplot2::geom_col(show.legend = FALSE) +
  ggplot2::facet_wrap(~ topic, scales = "free") +
  ggplot2::coord_flip() + 
  theme(axis.text.x = element_text(angle = 270))

```

```{r, ggsave-{{date}}}
ggplot2::ggsave(paste0("../results/beta-", "{{date}}", ".png"))
```


```{r, sleep-{{date}}}
Sys.sleep(15 * 60) # sleep for 15 minutes to allow api limits to reset
```

