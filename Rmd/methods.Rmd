# Methods

We designed and implemented a 
framework for mentored undergraduate data science research 
projects with big data. Below, we describe our initial framework 
and connect it to ideas from @nolan2010computing.

```{r t1, echo = FALSE}
library(magrittr)
step <- c(rep("Question formulation", 3),
          rep("Data acquisition", 3), 
          rep("Data analysis and visualization", 2), 
          rep("Presentation and communication", 3)
)
details <- c("List ideas", "Prioritize ideas", "Check data availability",
             "Download tweets and other data", "Parse tweet JSON", "Organize data as times series",
             "Exploratory plots", "Summary statistics",
             "Interpret and summarize graphs and summary statistics", "Prepare written materials and figures", "Share findings as posters, slides, and reports"
             )
tibble::tibble(step, details) %>%
#  dplyr::group_by(step) %>%
  gt::gt() %>%
  gt::tab_style(
    style = list(
      gt::cell_fill(color = "lightcyan"),
      gt::cell_text(style = "italic")
      ),
    locations = gt::cells_data(
      columns = dplyr::vars(details),
      rows = step == "Question formulation"
      )
  ) %>%
    gt::tab_style(
    style = list(
      gt::cell_fill(color = "pink"),
      gt::cell_text(style = "italic")
      ),
    locations = gt::cells_data(
      columns = dplyr::vars(details),
      rows = step == "Data acquisition"
      )
  ) %>%
  gt::tab_header(
    title = gt::md("Framework Overview"), subtitle = "xxx"
  ) %>%
  gt::tab_style(
    style = list(
      gt::cell_fill(color = "apricot"),
      gt::cell_text(style = "italic")
      ),
    locations = gt::cells_data(
      columns = dplyr::vars(details),
      rows = step == "Data analysis and visualization"
      )
  ) %>%
  gt::as_latex()
```




## Framework implementation


### Research question formulation


Our mentored research framework begins with brainstorming scientific research ideas 
based on the student's interests. This enables us to craft a project that excites the 
student. With the results of brainstorming sessions, we (mentors and student together) 
formulate the most promising ideas into scientific hypotheses. 

For the most appealing scientific 
hypotheses, we encourage the student to translate the scientific question into a 
statistical question that may be addressed with data. This is a crucial step in 
data science research question formulation. Skill in translating in both directions between 
scientific and statistical questions is a key communication skill that data science 
researchers offer.

### Data acquisition

We also incorporated data availability into our question formulation. 
We limited questions to those 
that could be studied with publicly available data. 
This practice also enabled reproducibility of
our analyses, since students could share the URL from which they accessed data.

Our computational system for acquiring tweets involved several steps. We interacted 
with the API via the R package `twitteR` [@twitteR]. We used the free Twitter streaming 
API that gave us access to approximately one percent stream of all tweets during the 
specified query time period. To ensure that we collected tweets continuously, we used 
the linux tool `crontab` to execute our R script every five minutes. Each execution of 
the R script performed a single streaming API query for five minutes. Twitter's 
streaming API, at the time of our data collection, enforced rate limits on the 
frequency and duration of queries. With the above settings, we continuously 
collected tweets. 

We encouraged students to complement tweets with additional data from publicly 
available sources.



### Data analysis and visualization

After identifying research questions and publicly available data, 
the next step is to decide on informative data visualizations and quantitative analyses. 
Because 
both projects involved exploratory analyses of times series, 
we encouraged students to think about
visualizations that might reveal relationships over time.

In the case of the event detection project, our student created 
word clouds for every inferred "topic". 
He also presented most probable words from each inferred topic, i.e., each distribution over words, as a bar plot.

The student working on sentiment analysis and market index prices plotted a 
daily sentiment "score" over time and presented it beside a plot of daily market 
index prices and compared the two plots.



### Presentation and communication of results

Students presented their research in a variety of settings. 
Each student presented at the annual undergraduate statistics poster session. 
We also encouraged them to present at the annual university-wide 
undergraduate research symposium.

In planning with students for poster and slide presentations, 
we (Hanlon and Boehm) emphasized
the importance of succinctly stating the research question and its scientific 
context. After clarifying the importance of the question, the student could 
proceed with explaining many of the elements that we've described above. Namely, the 
student would discuss the analyzed data and its acquisition while noting any shortcomings or 
biases of the data. For oral presentations, we suggested that 
students cautiously limit discussion of statistical methods, with the caveat that 
they prepare to answer detailed methodological inquiries during the question and 
answer session. 
Our students created powerful data 
visualizations for their projects. Their presentations also included their major results and future research directions.

In efforts to develop student written communication, we encouraged both students to 
prepare a written senior thesis document that detailed their research. In the senior 
thesis, we suggested that the students describe in rigorous detail their statistical 
methods. The rationale for this distinction, relative to the oral presentation, is 
that a reader doesn't have access to a question and answer session, while a poster 
session attendee may freely ask questions of the author.







## Examples

Examples may help to demonstrate our approach to identifying a statistical research 
question. One of our students had interests in acquiring and using social media posts. 
We helped her in brainstorming ideas for research involving social media sources like Facebook 
and Twitter. Through this brainstorming, we recognized that she had a parallel interest 
in financial markets. Our student hypothesized that sentiment analysis of finance-related tweets 
might reflect trends in financial market index prices. On days when the market index
prices increase, sentiment analysis of finance-related tweets might reveal more use of 
positive words, while days with decreasing prices might have more negative words 
in finance-related tweets.

A second student wanted to study tweets over time and entertainment events that 
garner lots of attention in social media. We encouraged this student to develop a 
strategy for event detection from tweets over time. The rationale is that 
a big entertainment event, such as the National Football League's Super Bowl game, 
might generate enough tweets that Super Bowl-related words would appear with high 
weights in results from latent Dirichlet allocation modeling of collections of tweets 
at distinct time points. We reasoned that Super Bowl-related topics might appear during 
the Super Bowl and vanish soon after the game's conclusion.



## Relating to three ideas from @nolan2010computing

We incorporate three key aspects that @nolan2010computing identified: 

1. broaden statistical computing to include emerging areas
1. deepen computational reasoning skills
1. combine computational topics with data analysis in the practice of statistics

Additionally, our projects gave students opportunities to develop and to practice skills in reproducible 
research. Given the growing imperative to document and share code to promote open science, we feel that 
this skill set equals in importance the three points above.

4. develop and practice skills in reproducible research to promote open science

Below, we describe how our framework enabled students to achieve competence in the four areas listed 
above.

## Broaden statistical computing to include emerging areas

Our framework broadens statistical computing by including the emerging areas of
social media data analysis, sentiment analysis, and topic modeling. 
Both students used Twitter tweets, which we accessed through a Twitter 
streaming API. One student chose to apply topic modeling to tweets. 
The other elected to apply sentiment analysis to daily 
tweet collections. By using these methods, sentiment analysis and 
topic modeling, students learned an aspect of text analysis that 
is not taught in traditional statistics courses.





## Deepen computational reasoning skills

Our framework encourages students to deepen 
computational reasoning skills in several ways. First, they work with a variety of 
internet-based data to answer research questions. In the two example cases, our 
students collected tweets over time and gathered complementary data from other 
resources, including daily closing prices of stock market indexes. This gave students 
opportunities to think creatively about what data to acquire and how to use multiple 
data sources in a single cohesive project. 

Second, the students worked with a variety of data structures. 
The Twitter streaming 
API returns tweets as JSON (Javascript Object Notation). Because 
distinct Twitter users may provide 
different pieces of profile information, there is variability in the structure of 
each tweet's JSON. 
Additionally, tweet metadata fields may appear in any order (https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data). 
Students needed to recognize this and to write code that 
accommodated these variations in tweet data structure.
Additional variability in tweet structure arose due to changes in the API. 
The evolving nature of JSON tweet structure 
(https://developer.twitter.com/en/docs/tweets/data-dictionary/guides/tweet-timeline) 
required students to write flexible code that could incorporate newly introduced 
or deprecated metadata. 


Students wrote R code to parse and organize tweet JSON. 
They organized their R code into a package, and shared it on Github (https://github.com/rturn/parseTweetFiles). 
Each tweet's JSON included required fields, and, possibly, some optional fields. 
Thus, students' code needed to accommodate variability in tweet structure.





## Combine computational topics with data analysis in the practice of statistics


Both mentored students combined computing with data analysis in 
the practice of statistics. 
They used a combination of latent dirichlet allocation topic 
modeling, sentiment analysis, and time series analysis to reach 
conclusions about real world phenomena.

Both drew heavily on the collection of tweets. One student examined 
Standard and Poor's 500 index daily 
closing prices over time. She also analyzed sentiments 
from each day's stock market-related tweets 
to look for relationships between tweet sentiment and stock market prices. 


Our other student focused on developing detection methods for social media events through 
topic modeling of tweets at different time periods. As a proof of principle, he fitted topic 
models to collections of tweets preceding, during, and following the National Football 
League's Super Bowl game. He hypothesized that topics would evolve 
over time, with football-related tweets appearing during the football 
game and disappearing soon after conclusion of the game.

Both students analyzed tweets as texts. This first required them to write code to parse the JSON
that the API returns. Once they had isolated the tweet text from its metadata, they parsed the 
tweet text into words for use in sentiment analysis and topic modeling. 
For the stock market project, they analyzed only those tweets that contained finance-related 
keywords. Sentiment analysis involved comparisons of tweet words to a dictionary that mapped words to 
sentiments. This yielded a net sentiment score for each tweet. 
They then treated tweet sentiment scores as a time series and compared them with 
daily stock market index closing prices.

The second student project involved latent Dirichlet allocation modeling of
tweet words at distinct time points to detect social media events [@blei2003latent]. 
Latent Dirichlet allocation is a bayesian nonparametric method for modeling text corpora as the result 
of words chosen from topics. The student treated tweets as "documents" (in the parlance of 
topic modeling). The goal of topic modeling, then, was to infer the underlying "topics" (or probability distributions over words) from a collection of tweets. 




    
## Develop and practice skills in reproducible research to promote open science

With the goal of promoting transparency in our research, we encouraged students to 
use `git` for version control of their code and documents and to share their code via
the website Github (https://github.com). One student also enrolled in Karl Broman's class on 
tools for reproducible research. This class features `git` and Github throughout 
its lectures and activities.

As we stated above, the students created an R package, `parseTweetFiles`, version controlled it with `git`, and shared it via Github.




