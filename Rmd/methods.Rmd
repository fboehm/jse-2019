# Methods

We designed and implemented a 
framework for mentored undergraduate data science research 
projects with big data. Below, we describe our initial framework 
and connect it to ideas from @nolan2010computing.


## Framework implementation


Our mentored research framework begins with brainstorming scientific research ideas 
based on the student's interests. This enables us to craft a project that excites the 
student. With the results of brainstorming sessions, we (mentors and student together) 
formulate the most promising ideas into scientific hypotheses. 

For the most appealing scientific 
hypotheses, we encourage the student to translate the scientific question into a 
statistical question that may be addressed with data. This is a crucial step in 
data science research question formulation. Skill in translating in both directions between 
scientific and statistical questions is a key communication skill that data science 
researchers offer.

We also incorporated data availability into our question formulation. 
We limited questions to those 
that could be studied with publicly available data. 
This practice also enabled reproducibility of
our analyses, since students could share the URL from which they accessed data.

After identifying research questions and publicly available data, 
the next step is to decide on informative data visualizations and quantitative analyses. 
Because 
both projects primarily involved exploratory analyses of times series, 
we encouraged students to think about
visualizations that might reveal relationships over time.


### Examples

Examples may help to demonstrate our approach to identifying a statistical research 
question. One of our students had interests in acquiring and using social media posts. 
We helped her in brainstorming ideas for research involving social media sources like Facebook 
and Twitter. Through this brainstorming, we recognized that she had a parallel interest 
in financial markets. Our student hypothesized that sentiment analysis of finance-related tweets 
might reflect trends in financial market index prices. On days when the market index
prices increase, sentiment analysis of finance-related tweets might reveal more use of 
positive words, while days with decreasing prices might have more negative words 
in finance-related tweets.

A second student wanted to study tweets over time and entertainment events that 
garner lots of attention in social media. We encouraged this student to develop a 
strategy for event detection from tweets over time. The rationale is that 
a big entertainment event, such as the National Football League's Super Bowl game, 
might generate enough tweets that Super Bowl-related words would appear with high 
weights in results from latent Dirichlet allocation modeling of collections of tweets 
at distinct time points. We reasoned that Super Bowl-related topics might appear during 
the Super Bowl and vanish soon after the game's conclusion.



## Relating to three ideas from @nolan2010computing

We incorporate three key aspects that @nolan2010computing identified: 

1. broaden statistical computing to include emerging areas
1. deepen computational reasoning skills
1. combine computational topics with data analysis in the practice of statistics

Additionally, our projects gave students opportunities to develop and to practice skills in reproducible 
research. Given the growing imperative to document and share code to promote open science, we feel that 
this skill set equals in importance the three points above.

4. develop and practice skills in reproducible research to promote open science

Below, we describe how our framework enabled students to achieve competence in the four areas listed 
above.

## Broaden statistical computing to include emerging areas

Our framework broadens statistical computing by including the emerging areas of
social media data analysis, sentiment analysis, and topic modeling. 
Both students used Twitter tweets, which we accessed through a Twitter 
streaming API. 

Our computational system for acquiring tweets involved several steps. We interacted 
with the API via the R package `twitteR` [@twitteR]. We used the free Twitter streaming 
API that gave us access to approximately one percent stream of all tweets during the 
specified query time period. To ensure that we collected tweets continuously, we used 
the linux tool `crontab` to execute our R script every five minutes. Each execution of 
the R script performed a single streaming API query for five minutes. Twitter's 
streaming API, at the time of our data collection, enforced rate limits on the 
frequency and duration of queries. With the above settings, we continuously collected 
tweets. 





## Deepen computational reasoning skills

Our framework encourages students to deepen 
computational reasoning skills in several ways. First, they work with a variety of 
internet-based data to answer research questions. In the two example cases, our 
students collected tweets over time and gathered complementary data from other 
resources, including daily closing prices of stock market indexes. This gave students 
opportunities to think creatively about what data to acquire and how to use multiple 
data sources in a single cohesive project. 

Second, the students worked with a variety of data structures. 
The Twitter streaming 
API returns tweets as JSON (Javascript Object Notation). Because 
distinct Twitter users may provide 
different pieces of profile information, there is variability in the structure of 
each tweet's JSON. 
Additionally, tweet metadata fields may appear in any order (https://developer.twitter.com/en/docs/tutorials/consuming-streaming-data). 
Students needed to recognize this and to write code that 
accommodated these variations in tweet data structure.
Additional variability in tweet structure arose due to changes in the API. 
The evolving nature of JSON tweet structure 
(https://developer.twitter.com/en/docs/tweets/data-dictionary/guides/tweet-timeline) 
required students to write flexible code that could incorporate newly introduced 
or deprecated metadata. 


Students wrote R code to parse and organize tweet JSON []. They organized their R code into a package, and shared it on Github (https://github.com/rturn/parseTweetFiles). 
Each tweet's JSON included required fields, and, possibly, some optional fields. 



## Combine computational topics with data analysis in the practice of statistics

computational topics: LDA & topic modeling; time series analysis; sentiment analysis.

Both mentored students combined computing with data analysis in 
the practice of statistics. 
They used a combination of latent dirichlet allocation topic 
modeling, sentiment analysis, and time series analysis to reach 
conclusions about real world data.

Both drew heavily on the collection of tweets. One student examined 
Standard and Poor's 500 index daily 
closing prices over time. She also analyzed sentiments 
from each day's stock market-related tweets 
to look for relationships between tweet sentiment and stock market prices. 


Our other student focused on developing detection methods for social media events through 
topic modeling of tweets at different time periods. As a proof of principle, he fitted topic 
models to collections of tweets preceding, during, and following the National Football 
League's Super Bowl game. He hypothesized that topics would evolve 
over time, with football-related tweets appearing during the football 
game and disappearing soon after conclusion of the game.

Both students analyzed tweets as texts. This first required them to write code to parse the JSON
that the API returns. Once they had isolated the tweet text from its metadata, they parsed the 
tweet text into words for use in sentiment analysis and topic modeling. 
For the stock market project, they analyzed only those tweets that contained finance-related 
keywords. Sentiment analysis involved comparisons of tweet words to a dictionary that mapped words to 
sentiments. This yielded a net sentiment score for each tweet. 
They then treated tweet sentiment scores as a time series and compared them with 
daily stock market index closing prices.

The second student project involved latent Dirichlet allocation modeling of
tweet words at distinct time points to detect social media events [@blei2003latent]. 
Latent Dirichlet allocation is a bayesian nonparametric method for modeling text corpora as the result 
of words chosen from topics. 



Our framework for mentored student research projects involves these components:

    

