---
title: "Tweets"
author: "Frederick J. Boehm"
date: "4/9/2020"
output: html_document
---

## Goals

1. Write a R script to query twitter streaming api to get 1 percent sample over 5'.
1. Demonstrate how to set up a crontab to execute script every 5'.
1. Verify that crontab approach works by running it on my computer for, say, 15'.
    1. Cautions about twitter databases: deletions and amendments to tweets
1. Examine structure of JSON obtained from api. 
1. Create phony JSON to represent a collection of tweets.
    1. Use lorem ipsum for text of tweets

## R code to query twitter streaming api

Install `rtweet` package from CRAN.

```{r, eval = FALSE}
install.packages("rtweet")
install.packages("lubridate")
```

```{r}
rtweet::stream_tweets(
  timeout = (15),
  parse = FALSE,
  file_name = paste0("../data/", lubridate::now(), "-tweets")
)
```

## Parse json file

```{r}
library(magrittr)
source("https://gist.githubusercontent.com/JBGruber/dee4c44e7d38d537426f57ba1e4f84ab/raw/ab87bebb8d020c2f96c71a40a483dc96a4c80e54/recover_stream.R")
```





```{r}
tweets <- recover_stream(path = "../data/2020-05-10 10:36:09-tweets.json", verbose = TRUE) %>%
  dplyr::filter(lang == "en") %>%
  tidytext::unnest_tokens(output = word, input = text)
```

## Prepare document-term matrix from tweet words

```{r}
# create a twitter-specific stop word list
twitter_stop <- tibble::tibble(word = c("https", "http", "t.co"))
# get a list of english language words and names
library(qdapDictionaries) # contains object GradyAugmented
```

```{r}
# https://www.tidytextmining.com/topicmodeling.html#library-heist
library(tidytext)
word_counts <- tweets %>% 
  dplyr::select(status_id, word) %>%
  dplyr::anti_join(stop_words) %>%
  dplyr::anti_join(twitter_stop) %>%
  dplyr::filter(word %in% GradyAugmented) %>%
  dplyr::count(status_id, word, sort = TRUE) %>%
  dplyr::ungroup()
```

```{r}
tweets_dtm <- word_counts %>%
  tidytext::cast_dtm(status_id, word, n)
```

```{r}
tweets_lda <- topicmodels::LDA(tweets_dtm, k = 8)
```

```{r}
tweets_beta <- tidytext::tidy(tweets_lda, matrix = "beta")
```

```{r}
tweets_top_terms <- tweets_beta %>%
  dplyr::group_by(topic) %>%
  dplyr::top_n(10, beta) %>%
  dplyr::ungroup() %>%
  dplyr::arrange(topic, -beta)
```

```{r}
tweets_top_terms %>%
  dplyr::mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot2::ggplot(ggplot2::aes(term, beta, fill = factor(topic))) +
  ggplot2::geom_col(show.legend = FALSE) +
  ggplot2::facet_wrap(~ topic, scales = "free") +
  ggplot2::coord_flip()
```

```{r}
tweets_gamma <- tidytext::tidy(tweets_lda, matrix = "gamma")
tweets_gamma
```

```{r, eval = FALSE}
# too many documents to plot all
tweets_gamma %>%
  dplyr::mutate(title = reorder(document, gamma * topic)) %>%
  ggplot2::ggplot(ggplot2::aes(factor(topic), gamma)) +
  ggplot2::geom_boxplot() +
  ggplot2::facet_wrap(~ document)
```





### Colophon

This report was generated on `r Sys.time()` using the following computational environment and dependencies: 

```{r colophon, cache = FALSE}
# which R packages and versions?
if ("devtools" %in% installed.packages()) devtools::session_info()
```

The current Git commit details are:

```{r}
# what commit is this file at? 
if ("git2r" %in% installed.packages() & git2r::in_repository(path = ".")) git2r::repository(here::here())  
```
