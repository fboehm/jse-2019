---
title: 'Analyzing tweets to detect social media events'
author: "Frederick J. Boehm and Bret M. Hanlon"
date: "June 15, 2020"
params:
  lastmod: !r lubridate::now()
bibliography: jse.bib
csl: chicago-annotated-bibliography.csl
geometry: "left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm"
output: 
  bookdown::pdf_book:
    toc: FALSE
    number_sections: FALSE
    includes:
      in_header: head.tex
  bookdown::word_document2:
    toc: FALSE
---


```{r child = "abstract.Rmd", eval = FALSE}
```

# Introduction

Twitter has profoundly changed how we communicate. 
In only 280 characters, users can instantly contribute to public conversations on politics, 
current events, sports, media, and many other topics. 
Recent development of accessible statistical methods for large-scale text 
analysis now enable instructors to use tweets as contemporary pedagogical 
tools in guiding undergraduate research projects.
We report one instance of a mentored text analysis research project. 
We share our data and computer code to encourage others to undertake 
tweet text analysis research. We also describe our methods for creating a 
collection of tweets.

Some social media data, including tweets from Twitter, is
available through website application product interfaces (APIs). 
By way of a streaming API, Twitter shares a sample of approximately one percent of all 
tweets during an API query time period [@tweet_stream]. Any Twitter user can freely 
access this one percent sample, whereas access to a larger selection is available to researchers for a fee.



Using large collections of tweets, scholars have studied diverse research questions, including
the inference of relationships and social networks among Twitter users [@lin2011joint]; 
authorship of specific tweets when multiple persons share a single account [@drob]; and 
rhetoric in recruiting political supporters [@pelled2018little;@wells2016trump].
Recognizing the potential utility of tweets for data science research and teaching, 
we created a collection of tweets over time by repeated querying of the Twitter streaming API.

In line with recent calls for students to work with real data [@nolan2010computing], our 
collection of tweets has served as a valuable resource in our mentoring 
of undergraduate data science research. 
Working with real data allows students to develop proficiency not 
only in statistical analysis,
but also in related data science skills, including data 
transfer from online sources, data storage, using data 
from multiple file formats, and communicating findings and their limitations. 
Collaboratively asking and addressing novel questions with 
our collection of tweets gave mentored students 
opportunities to develop competency in all of these areas.



While our tweet collection enables us to address many possible research questions, the 
dynamic content of tweets over time particularly piqued our interest. Together, students and 
mentors hypothesized that high-profile social media events would generate a high volume of 
tweets, and that we would detect social media events through changes in tweet 
topic content over time. We present below 1) an approach for collecting tweets in real 
time and 2) statistical methods for detecting social media 
events via latent Dirichlet allocation modeling of collections of tweets and 3) reflections on 
using this data set in research mentoring of undergraduate students.




## Methods



### Collecting tweets over time

We include here instructions for creating a tweet collection. First, we created a 
new account on Twitter. With these user credentials, we used the 
R package `rtweet` to query the API. Because we work with linux operating systems, we used the `crontab` software 
to repeatedly execute R code to submit API queries. Each 
query lasted five minutes. We timed the API queries so 
that there was no time lag between queries. We stored tweets resulting from API queries in their native JSON format. 

The R package `rtweet` 
provides functions that parse tweet JSON to R data frames. We then 
conducted all further analyses in R.

Setting up the query task with `crontab` is straightforward. On our computer, with 
Ubuntu 20.04 linux operating system, we opened a terminal and typed `crontab -e`. This 
opened a text file containing user-specified tasks. We added the following line to the 
bottom of the file:

```{bash, eval = FALSE}
*/5 * * * * R -e 'rtweet::stream_tweets(timeout = (60 * 5), 
parse = FALSE, file_name = paste0("~/work/mentoring/mentoring-framework/data/",
lubridate::now(), "-tweets"))'
```

Users may need to slightly amend the above line to conform to requirements of their operating system's `crontab`.

### Querying Twitter API to get complete tweets

Twitter API use agreements forbid users from sharing complete API query results. 
However, Twitter enables users to share tweet identification numbers. A user can 
then query a Twitter API to obtain complete tweet data. In our experience, this 
process is incomplete; that is, many tweets submitted to the Twitter API return 
no data. Additionally, on repeated querying of the API, different sets of tweets 
return data. This complicates our goal of making all analyses computationally 
reproducible. 

From our collection of tweets, we chose to analyze those sent on five consecutive 
days from May 10, 2020 to May 14, 2020. We then submitted API queries to Twitter
to get the full content of tweets, including the tweet text.
We provide below the R code that we used to query the Twitter API to obtain full tweet content. 



### Tweet structure

Tweets are available as Javascript Object Notation (JSON) objects. Every tweet consists 
of multiple named fields, each of which is a key-value pair. The number of fields per 
tweet depends on user settings, retweet status, and other factors [@tweet_json]. 

```{bash, eval = FALSE}
{
  "created_at": "Thu Apr 06 15:24:15 +0000 2017",
  "id_str": "850006245121695744",
  "text": "1\/ Today we\u2019re sharing our vision for the future of the Twitter API platform!\nhttps:\/\/t.co\/XweGngmxlP",
  "user": {
    "id": 2244994945,
    "name": "Twitter Dev",
    "screen_name": "TwitterDev",
    "location": "Internet",
    "url": "https:\/\/dev.twitter.com\/",
    "description": "Your official source for Twitter Platform news, updates & events. 
    Need technical help? Visit https:\/\/twittercommunity.com\/ \u2328\ufe0f 
    #TapIntoTwitter"
  },
  "place": {   
  },
  "entities": {
    "hashtags": [      
    ],
    "urls": [
      {
        "url": "https:\/\/t.co\/XweGngmxlP",
        "unwound": {
          "url": "https:\/\/cards.twitter.com\/cards\/18ce53wgo4h\/3xo1c",
          "title": "Building the Future of the Twitter API Platform"
        }
      }
    ],
    "user_mentions": [     
    ]
  }
}
```


### Parsing text of tweets

We used functions from the `rtweet` R package to parse tweet JSON into a data frame. From there, we used `tidytext` R package 
functions to break the tweet text into individual words. We discarded commonly used "stop words" 
and emojis. 

Latent Dirichlet allocation models require that the corpus be inputted as a 
document by term matrix. Each row corresponds to a single document (a single 
tweet), and each column is a single term (or word). Each cell contains a 
count (the number of 
occurrences of a term in the specified document). We created a document by 
term matrix with the R functions `` from the `` R package.

### Latent Dirichlet allocation

Latent Dirichlet allocation is a statistical method for inferring latent (unobservable) topics (or themes)
from a collection (or corpus) of documents. We pretend that
there's an imaginary process for creating documents in the corpus. For each document,
we choose a discrete distribution over topics. For example, some Mother's Day tweets wish 
mothers a happy celebration. This may constitute one topic in the corpus. 
Having chosen a distribution over topics, we then select document words by first 
drawing a topic from the distribution over topics, then drawing a word from the
chosen topic. 
Thus, a topic is technically defined as a distribution over words in a fixed vocabulary (or collection of words).


The literature on latent Dirichlet allocation and related methods is vast, 
and we won't attempt to review it here. 




### Study design

We sought to validate our hypothesis that we could detect a major social 
media event by examining tweet topic content at distinct time periods. 
As a proof of principle of our event detection strategy, we chose to 
analyze tweets during and after Mother's Day 2020. We fitted latent Dirichlet allocation models for each of four 
distinct five-minute periods. The first period began at noon Eastern time on Mother's Day 2020. Subsequent time periods started 
24, 48, and 72 hours later. 
We defined each time period to be a single collection, or corpus, of tweets. We then fitted latent 
Dirichlet allocation models to each corpus. 

We used several criteria to evaluate latent Dirichlet allocation model fits, with 
emphasis on choosing a reasonable number of topics per corpus.
Our strategy involved both visualization and more quantitative approaches to model 
evaluation. For every model, we created one word cloud per topic. 

We then inspected topic contents at each of the four time points.











```{r child = "results.Rmd"}
```


```{r child = "discussion.Rmd"}
```








## References


