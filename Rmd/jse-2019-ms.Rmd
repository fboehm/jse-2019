---
title: 'Detecting social media events through tweet content analysis'
author: "Frederick J. Boehm and Bret M. Hanlon"
date: "June 15, 2020"
params:
  lastmod: !r lubridate::now()
bibliography: jse.bib
geometry: "left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm"
output: 
  bookdown::pdf_book:
    toc: FALSE
    number_sections: TRUE
    includes:
      in_header: head.tex
  bookdown::word_document2:
    toc: FALSE
---


```{r child = "abstract.Rmd", eval = FALSE}
```

# Introduction

Twitter has profoundly changed how we communicate. 
In only 280 characters, users can instantly contribute to public conversations on politics, 
current events, sports, media, and many other topics. 
Recent development of accessible statistical methods for text 
analysis now enable mentors to use tweets as contemporary pedagogical 
tools in guiding undergraduate research projects. 

Some social media data, including tweets from Twitter, is
available through website application product interfaces (APIs). 
By way of a streaming API, Twitter shares a sample of approximately one percent of all 
tweets during an API query time period [@tweet_stream]. Any Twitter user can freely 
access this one percent sample, whereas access to a larger selection is available to researchers for a fee.



Using large collections of tweets, researchers have studied
inference of relationships and social networks among Twitter users [@lin2011joint]; 
authorship of specific tweets when multiple persons share a single account [@drob]; and 
rhetoric in recruiting political supporters [@pelled2018little;@wells2016trump].
Recognizing the potential utility of tweets for data science research and teaching, 
we created a collection of tweets over time by repeated querying of the Twitter streaming API.

In line with recent calls for students to work with real data [@nolan2010computing],   our collection of tweets has served as a valuable resource in our mentoring 
of undergraduate data science research. 
Working with real data allows students to develop proficiency not 
only in statistical analysis,
but also in related data science skills, including data 
transfer from online sources, data storage, and using data 
from multiple file formats. Collaboratively asking and addressing novel questions with 
our collection of tweets gave mentored students 
opportunities to develop competency in all of these areas.

Mentoring in the work place and in higher education has many 
benefits, including improving 1) students' development as thinkers and scholars, 2) confidence in their 
own abilities, 3) integration into the campus community,
and 4) interest in graduate training [@baker2010beyond;@higgins2001reconceptualizing]. 
We strived to develop trusting, mutually respectful mentoring relationships with 
our students while advising their senior projects. With input from us,
the students selected appealing research questions. 



While our tweet collection enables us to address many possible research questions, the 
content of tweets over time particularly intrigued us. We 
hypothesized that high-profile social media events would generate a high volume of 
tweets, and that we'd detect social media events through changes in tweet 
topic content over time. We present below 1) an approach for collecting tweets in real 
time and 2) statistical methods for detecting social media 
events via latent Dirichlet allocation modeling of collections of tweets and 3) reflections on 
using this data set in research mentoring of undergraduate 
students.




## Methods

### Study design

We sought to validate our hypothesis that we could detect a major social 
media event by examining tweet topic content at distinct time periods. 
As a proof of principle of our event detection strategy, we chose to 
analyze tweets before, during, and after the National Football League's 
2015 Super Bowl. We fitted latent Dirichlet allocation models for each of five 
distinct one-hour periods. The first period began approximately 48h 
before the Super Bowl halftime show. Subsequent time periods started at -24, 
0, +24, and +48 hours after the approximate start time of the halftime show. 
We defined each time period to be a single collection, or corpus, of tweets. Each tweet 
constituted a "document", in the terminology of text analysis. We then fitted latent 
Dirichlet allocation models to each of the five corpora. 



### Collecting tweets over time

We include here instructions for creating a tweet collection. First, we created a 
new account on Twitter. With these user credentials, we used the 
R package `rtweet` to query the API. Because we work on computers 
with linux operating systems, we use the linux software 
`crontab` to repeatedly execute R code to submit API queries. Each 
query lasts a user-specified duration. We time the API queries so 
that there is no time lag between queries. We store API query 
results in their native JSON format. The R package `rtweet` 
provides functions that parse tweet JSON to R data frames. We then 
conducted all further analyses with the R data frames.

### Querying Twitter API to get complete tweets

We queried a tweets database, created with the methods described above, to get ID numbers for 
tweets from the desired time periods. We then submitted API queries to Twitter
to get the full content of the tweets, including the tweet text.
We provide below the R code that we used to query the Twitter API to obtain full tweet content. 

```{r, eval = FALSE}
rtweet::lookup_tweets()
```

### Tweet structure

Tweets are available as Javascript Object Notation (JSON) objects. Every tweet consists of multiple named fields, each of which is a key-value pair. The number of fields per tweet depends on user settings, retweet status, and other factors. 

**PLACE TWEET JSON HERE**



### Parsing text of tweets

We used functions from the `rtweet` package to parse tweet JSON into a data frame. From there, we used `tidytext` R package functions to break the tweet text into individual words for latent Dirichlet allocation. We discarded commonly used "stop words" and emojis. 

Latent Dirichlet allocation models require that the corpus be inputted as a 
document by term matrix. Each row corresponds to a single document (a single 
tweet), and each column is a single term (or word). Each cell contains a 
count (the number of 
occurrences of a term in the specified document). We created a document by 
term matrix with the R functions `` from the `` R package.










```{r child = "results.Rmd"}
```


```{r child = "discussion.Rmd"}
```








## References


